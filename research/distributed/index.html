<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Distributed learning | Carlo Masone </title> <meta name="author" content="Carlo Masone"> <meta name="description" content="(Learn) One for all, and all for one."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/my_icon.png?dd27a82351df81a65f9fc1dac22bddb7"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://cmas1.github.io/research/distributed/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Carlo</span> Masone </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/research/">research <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">people </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/phd_students/">phd students</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/master_students/">master students</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/collaborators/">collaborators</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">open positions </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/theses/">theses</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/phd_positions/">phd positions</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/senior%20positions/">senior positions</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Distributed learning</h1> <p class="post-description">(Learn) One for all, and all for one.</p> </header> <article> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research/distributed/distributed_idea-480.webp 480w,/assets/img/research/distributed/distributed_idea-800.webp 800w,/assets/img/research/distributed/distributed_idea-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/research/distributed/distributed_idea.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="learning together" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 1.</b> Learn one for all, and learn all for one. Image credits to Marvin Meyer <a href="https://unsplash.com/photos/people-sitting-down-near-table-with-assorted-laptop-computers-SYTO3xs06fU" rel="external nofollow noopener" target="_blank">Marvin Meyer</a>. </div> <p>In many technological fields, from engineering to computer science, there is an effort to move from centralized solutions, where there is a single system that owns the data, reasons and acts to solve a problem, towards decentralized solutions, where the task is aplit across multiple systems. This is also the case for machine learning. The term <strong>Distributed Machine Learning</strong> refers to the idea of distributing the process of learning from the data across multiple nodes (physical or virtual). There are a lot of nuances to this idea, for example in the nature of the nodes, their location, if they are heterogenous or not, if the serve the same function or not, in the location of the data and whether it can be shared or not, in the goal of the learning procedure (learn a single model, learn personalized models), if the system aims to speed up the training, etcetera.</p> <p>In <a href="http://vandal.polito.it/" rel="external nofollow noopener" target="_blank">Vandal</a> there are a few researchers working on this topic, and I have been collaborating on a few projects.</p> <h5 id="federated-learning">Federated learning</h5> <p><strong>Federated learning</strong> (also known as collaborative learning) is a sub-field the wider field of distributed machine learning focusing on settings in which multiple entities (often referred to as clients) collaboratively train a model while ensuring that their data remains decentralized (for example due to privacy constraints). The clients may also have limited computational power and availability to participate to the training. One of the primary defining characteristics of federated learning is data heterogeneity. Due to the decentralized nature of the clients’ data, there is no guarantee that data samples held by each client are independently and identically distributed.</p> <p>We are working on solutions that try to address the problem of data heterogeneity, to improve the model convergence and final model quality while being efficient in terms of communication with the aggregation server.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research/distributed/fedhbm_results-480.webp 480w,/assets/img/research/distributed/fedhbm_results-800.webp 800w,/assets/img/research/distributed/fedhbm_results-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/research/distributed/fedhbm_results.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="FedHBM" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 2.</b> Results obtained with our <a href="http://arxiv.org/abs/2404.13324" rel="external nofollow noopener" target="_blank">FedHBM and GHBM</a> methods. </div> <p>We have also worked on new applications for federated learning. In fact, many solutions developed for federated learning are tested and validated for image classification tasks and on small datasets and models. We have also proposed the first (to the best of our knowledge) application of <strong>federated learning for image retrieval</strong>, and more specifically to a visual geo-localization problem (where ther ecould be multiple robots/vehicles around the world collecting navigation data).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research/distributed/fedvpr_training-480.webp 480w,/assets/img/research/distributed/fedvpr_training-800.webp 800w,/assets/img/research/distributed/fedvpr_training-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/research/distributed/fedvpr_training.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Federated Visual Place Recognition" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 3.</b> Sketch of the training procedure for <a href="http://arxiv.org/abs/2404.13324" rel="external nofollow noopener" target="_blank">Federated Visual Place Recognition</a>. </div> <h5 id="training-parallelization">Training parallelization</h5> <p>Recent breakthroughs enabled by deep learning rely on increasingly complex architectures: the advent of Large Language Models (LLMs) has marked a new chapter for conversational agents like ChatGPT, but also introduced real challenges in training and serving such solutions for real-world applications. Therefore, scaling data and computing power are crucial for successful training, but speed-up scaling laws are not going to cope forever with the limits of current algorithms and architectures.</p> <p>We have been working on some projects related the parallelization and distribution of the training of these algorithm, in order to allow better scalability. One aspects that we have worked on, for the spcific application of visual place recognition, is on how to leverage a smarter data partition in order to improve and speed up the training (see Figure 4).</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research/distributed/dCosplace-480.webp 480w,/assets/img/research/distributed/dCosplace-800.webp 800w,/assets/img/research/distributed/dCosplace-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/research/distributed/dCosplace.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Distributed CosPlace" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 4.</b> Sketch of the <a href="http://arxiv.org/abs/2404.13324" rel="external nofollow noopener" target="_blank">Distributed CosPlace</a> algorithm for visual place recognition. </div> <hr> <h2>Related Publications</h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#b947f3"> <div>Journal</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Zaccone-2024-dCosplace-480.webp 480w,/assets/img/publication_preview/Zaccone-2024-dCosplace-800.webp 800w,/assets/img/publication_preview/Zaccone-2024-dCosplace-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/Zaccone-2024-dCosplace.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Zaccone-2024-dCosplace.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Zaccone-2024-dCosplace" class="col-sm-8"> <div class="title">Distributed training of CosPlace for large-scale visual place recognition</div> <div class="author"> Riccardo Zaccone, Gabriele Berton, and <em>Carlo Masone</em> </div> <div class="periodical"> <em>Frontiers in Robotics and AI</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2024.1386464/full" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="altmetric-embed" data-badge-type="2" data-badge-popover="right" data-altmetric-id="163643285"></span> <span class="__dimensions_badge_embed__" data-doi="10.3389/frobt.2024.1386464" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=cM3Iz_4AAAAJ&amp;citation_for_view=cM3Iz_4AAAAJ:maZDTaKrznsC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Visual place recognition (VPR) is a popular computer vision task aimed at recognizing the geographic location of a visual query, usually within a tolerance of a few meters. Modern approaches address VPR from an image retrieval standpoint using a kNN on top of embeddings extracted by a deep neural network from both the query and images in a database. Although most of these approaches rely on contrastive learning, which limits their ability to be trained on large-scale datasets (due to mining), the recently reported CosPlace proposes an alternative training paradigm using a classification task as the proxy. This has been shown to be effective in expanding the potential of VPR models to learn from large-scale and fine-grained datasets. In this work, we experimentally analyze CosPlace from a continual learning perspective and show that its sequential training procedure leads to suboptimal results. As a solution, we propose a different formulation that not only solves the pitfalls of the original training strategy effectively but also enables faster and more efficient distributed training. Finally, we discuss the open challenges in further speeding up large-scale image retrieval for VPR.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Zaccone-2024-dCosplace</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Distributed training of CosPlace for large-scale visual place recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zaccone, Riccardo and Berton, Gabriele and Masone, Carlo}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Frontiers in Robotics and AI}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-11}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Frontiers Media SA}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3389/frobt.2024.1386464}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{localization, spatial intelligence, distributed learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#dd7f7f"> <div>Workshop</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Dutto-2024-fedvpr-480.webp 480w,/assets/img/publication_preview/Dutto-2024-fedvpr-800.webp 800w,/assets/img/publication_preview/Dutto-2024-fedvpr-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/Dutto-2024-fedvpr.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Dutto-2024-fedvpr.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Dutto-2024-fedvpr" class="col-sm-8"> <div class="title">Collaborative Visual Place Recognition through Federated Learning</div> <div class="author"> Mattia Dutto, Gabriele Berton, Debora Caldarola, Eros Fanı̀, Gabriele Trivigno, and <em>Carlo Masone</em> </div> <div class="periodical"> <em>In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, Jun 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2404.13324" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content/CVPR2024W/FedVision-2024/html/Dutto_Collaborative_Visual_Place_Recognition_through_Federated_Learning_CVPRW_2024_paper.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=cM3Iz_4AAAAJ&amp;citation_for_view=cM3Iz_4AAAAJ:isC4tDSrTZIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Visual Place Recognition (VPR) aims to estimate the location of an image by treating it as a retrieval problem. VPR uses a database of geo-tagged images and leverages deep neural networks to extract a global representation called descriptor from each image. While the training data for VPR models often originates from diverse geographically scattered sources (geo-tagged images) the training process itself is typically assumed to be centralized. This research revisits the task of VPR through the lens of Federated Learning (FL) addressing several key challenges associated with this adaptation. VPR data inherently lacks well-defined classes and models are typically trained using contrastive learning which necessitates a data mining step on a centralized database. Additionally client devices in federated systems can be highly heterogeneous in terms of their processing capabilities. The proposed FedVPR framework not only presents a novel approach for VPR but also introduces a new challenging and realistic task for FL research. This has the potential to spur the application of FL to other image retrieval tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Dutto-2024-fedvpr</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Dutto, Mattia and Berton, Gabriele and Caldarola, Debora and Fan{\`\i}, Eros and Trivigno, Gabriele and Masone, Carlo}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Collaborative Visual Place Recognition through Federated Learning}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jun</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4215-4225}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{localization, spatial intelligence, distributed learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#2170b9"> <div>Preprint</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/Zaccone-2024-fedhbm-480.webp 480w,/assets/img/publication_preview/Zaccone-2024-fedhbm-800.webp 800w,/assets/img/publication_preview/Zaccone-2024-fedhbm-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/Zaccone-2024-fedhbm.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Zaccone-2024-fedhbm.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Zaccone-2024-fedhbm" class="col-sm-8"> <div class="title">Communication-Efficient Heterogeneous Federated Learning with Generalized Heavy-Ball Momentum</div> <div class="author"> Riccardo Zaccone, <em>Carlo Masone</em>, and Marco Ciccone </div> <div class="periodical"> Jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2311.18578" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-pmid="" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=cM3Iz_4AAAAJ&amp;citation_for_view=cM3Iz_4AAAAJ:RHpTSmoSYBkC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Federated Learning (FL) has emerged as the state-of-the-art approach for learning from decentralized data in privacy-constrained scenarios. However, system and statistical challenges hinder real-world applications, which demand efficient learning from edge devices and robustness to heterogeneity. Despite significant research efforts, existing approaches (i) are not sufficiently robust, (ii) do not perform well in large-scale scenarios, and (iii) are not communication efficient. In this work, we propose a novel Generalized Heavy-Ball Momentum (GHBM), motivating its principled application to counteract the effects of statistical heterogeneity in FL. Then, we present FedHBM as an adaptive, communication-efficient by-design instance of GHBM. Extensive experimentation on vision and language tasks, in both controlled and realistic large-scale scenarios, provides compelling evidence of substantial and consistent performance gains over the state of the art.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">Zaccone-2024-fedhbm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Communication-Efficient Heterogeneous Federated Learning with Generalized Heavy-Ball Momentum}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zaccone, Riccardo and Masone, Carlo and Ciccone, Marco}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{distributed learning}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Carlo Masone. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-research",title:"research",description:"A brief summary of my present and past research.",section:"Navigation",handler:()=>{window.location.href="/research/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order. generated by jekyll-scholar.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"cv",description:"Brief Curriculum Vitae. For the extended version, please check the pdf version.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-repositories",title:"repositories",description:"Repositories and software libraries.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"dropdown-phd-students",title:"phd students",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-master-students",title:"master students",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-collaborators",title:"collaborators",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-theses",title:"theses",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-phd-positions",title:"phd positions",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-senior-positions",title:"senior positions",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"nav-teaching",title:"teaching",description:"Teaching and courses.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/sample-posts/2024/05/01/tabs.html"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/sample-posts/2024/04/29/typograms.html"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/sample-posts/2024/04/28/post-citation.html"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/sample-posts/2024/04/15/pseudocode.html"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/sample-posts/2024/01/27/code-diff.html"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/sample-posts/2024/01/27/advanced-images.html"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/sample-posts/2024/01/27/vega-lite.html"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/sample-posts/2024/01/26/geojson-map.html"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/sample-posts/2024/01/26/echarts.html"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/sample-posts/2024/01/26/chartjs.html"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/sample-posts/2023/12/12/tikzjax.html"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/sample-posts/2023/07/12/post-bibliography.html"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/sample-posts/2023/07/04/jupyter-notebook.html"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/sample-posts/2023/05/12/custom-blockquotes.html"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/sample-posts/2023/04/25/sidebar-table-of-contents.html"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/sample-posts/2023/04/25/audios.html"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/sample-posts/2023/04/24/videos.html"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/sample-posts/2023/03/20/tables.html"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/sample-posts/2023/03/20/table-of-contents.html"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/sample-posts/external-services/2022/12/10/giscus-comments.html"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/2021/07/04/diagrams.html"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/2021/05/22/distill.html"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/sample-posts/external-services/2020/09/28/twitter.html"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/sample-posts/external-services/2015/10/20/disqus-comments.html"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/sample-posts/2015/10/20/math.html"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/sample-posts/2015/07/15/code.html"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/sample-posts/2015/05/15/images.html"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march & april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/sample-posts/2015/03/15/formatting-and-links.html"}},{id:"news-trophy-i-have-been-nominated-as-an-outstanding-reviewer-for-cvpr-2024-third-year-in-a-row-open-mouth",title:'<img class="emoji" title=":trophy:" alt=":trophy:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c6.png" height="20" width="20"> I have been nominated as an Outstanding Reviewer for CVPR 2024! Third...',description:"",section:"News"},{id:"news-scroll-our-paper-mask2anomaly-mask-transformer-for-universal-open-set-segmentation-was-accepted-to-ieee-transactions-on-pattern-analysis-and-machine-intelligence-kudos-to-shyam-nandan-rai-for-the-achievement",title:'<img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> Our paper Mask2Anomaly: Mask Transformer for Universal Open-set Segmentation was accepted to...',description:"",section:"News"},{id:"news-star-for-the-second-year-in-a-row-i-am-serving-as-area-chair-for-the-wacv-conference",title:'<img class="emoji" title=":star:" alt=":star:" src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png" height="20" width="20"> For the second year in a row, I am serving as Area...',description:"",section:"News"},{id:"news-scroll-our-paper-meshvpr-citywide-visual-place-recognition-using-3d-meshes-was-accepted-to-eccv-2024-check-out-the-project-page",title:'<img class="emoji" title=":scroll:" alt=":scroll:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4dc.png" height="20" width="20"> Our paper MeshVPR: Citywide Visual Place Recognition Using 3D Meshes was accepted...',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"research-cooperative-aerial-transportation",title:"Cooperative Aerial Transportation",description:"a cable-suspended payload transported by a team of micro UAVs",section:"Research",handler:()=>{window.location.href="/research/aerial_cable/"}},{id:"research-cablerobot-simulator",title:"CableRobot Simulator",description:"world's first cable robot for passengers",section:"Research",handler:()=>{window.location.href="/research/cable_robot/"}},{id:"research-continuous-kernels-learning",title:"Continuous kernels learning",description:"Coming soon.",section:"Research",handler:()=>{window.location.href="/research/continuous_function/"}},{id:"research-cybermotion-simulator",title:"CyberMotion Simulator",description:"a motion simulator based on an industrial robot arm",section:"Research",handler:()=>{window.location.href="/research/cyber_motion/"}},{id:"research-distributed-learning",title:"Distributed learning",description:"(Learn) One for all, and all for one.",section:"Research",handler:()=>{window.location.href="/research/distributed/"}},{id:"research-edge-ai",title:"Edge AI",description:"Coming soon.",section:"Research",handler:()=>{window.location.href="/research/edge_AI/"}},{id:"research-localization",title:"Localization",description:"Where am I? Where is everything else?",section:"Research",handler:()=>{window.location.href="/research/localization/"}},{id:"research-reliable-machine-learning",title:"Reliable machine learning",description:"Sorry, the AI is out of order. Please call the technician.",section:"Research",handler:()=>{window.location.href="/research/reliable_learning/"}},{id:"research-robust-control-of-robotic-platforms",title:"Robust Control of Robotic Platforms",description:"sliding mode controllers for robotic platforms",section:"Research",handler:()=>{window.location.href="/research/robust_control/"}},{id:"research-shared-control-and-planning-of-uavs",title:"Shared Control and Planning of UAVs",description:"control and planning algorithms for UAVs, whose execution is interfaced with a human operator.",section:"Research",handler:()=>{window.location.href="/research/shared_control/"}},{id:"research-fine-grained-visual-understanding",title:"Fine grained visual understanding",description:"from patch-level to pixel-level details",section:"Research",handler:()=>{window.location.href="/research/visual_understanding/"}},{id:"software-holorlib",title:"HolorLib",description:"A C++20 header-only library for generic multi-dimensional containers.",section:"Software",handler:()=>{window.location.href="/software/holorlib/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%63%61%72%6C%6F.%6D%61%73%6F%6E%65@%70%6F%6C%69%74%6F.%69%74","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0002-1609-9338","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=cM3Iz_4AAAAJ","_blank")}},{id:"socials-researchgate",title:"ResearchGate",section:"Socials",handler:()=>{window.open("https://www.researchgate.net/profile/Carlo-Masone/","_blank")}},{id:"socials-scopus",title:"Scopus",section:"Socials",handler:()=>{window.open("https://www.scopus.com/authid/detail.uri?authorId=36463980500","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/cmas1","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/cmasone","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/masone_carlo","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>